{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ci9sUfw7j55v"
      },
      "source": [
        "# **Configuration de PySpark**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXy8R8HSkL5y"
      },
      "source": [
        "Installation de Pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-0OsMQaIjeH_"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fu6ajoy7kHM2"
      },
      "source": [
        "# Installation de Findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5ycw8F6kBR6",
        "outputId": "b43ee228-97b8-4f9e-b8cd-d44f37e845f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting findspark\n",
            "  Downloading findspark-2.0.1-py2.py3-none-any.whl.metadata (352 bytes)\n",
            "Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
            "Installing collected packages: findspark\n",
            "Successfully installed findspark-2.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install findspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKGWaTVgkVHT"
      },
      "source": [
        "# Configurationde Spark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7Sj4CfthNnk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUygjCzkkZEy"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aypgwmL6wNCg"
      },
      "source": [
        "# Simulation d'un flux de données"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUCSFRJkv-k0"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import csv\n",
        "\n",
        "# Simuler un flux en écrivant périodiquement des lignes dans un fichier CSV\n",
        "def simulate_stream(file_name):\n",
        "    data = [\n",
        "        {\"name\": \"Alice\", \"age\": 22, \"major\": \"Engineering\"},\n",
        "        {\"name\": \"Bob\", \"age\": 20, \"major\": \"Science\"},\n",
        "        {\"name\": \"Charlie\", \"age\": 25, \"major\": \"Mathematics\"}\n",
        "    ]\n",
        "    with open(file_name, 'w', newline='') as file:\n",
        "        writer = csv.DictWriter(file, fieldnames=[\"name\", \"age\", \"major\"])\n",
        "        writer.writeheader()\n",
        "        for row in data:\n",
        "            writer.writerow(row)\n",
        "            time.sleep(2)  # Ajout d'une nouvelle ligne toutes les 2 secondes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yojwcmX_1bL9"
      },
      "source": [
        "# **Création d'une dataframe**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Luzs88jwxKuy",
        "outputId": "04c1b1c7-c6ca-4a6f-b61b-f09d765d01fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fichier students.csv créé !\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Données initiales\n",
        "data = {\n",
        "    \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\", \"Frank\", \"Grace\", \"Hank\", \" Irene\", \"Jack\", \"Karen\", \"Leo\", \"Mia\", \"Noah\", \"Tom\", \"Lucie\", \"Clara\", \"Bill\", \"Carl\", \"Patricia\"],\n",
        "    \"age\": [22, 20, 25, 19, 24, 23, 21, 26, 22, 20, 23, 24, 21, 25, 22, 20, 23, 25, 18, 20],\n",
        "    \"major\": [\"Engineering\", \"Science\", \"Mathematics\", \"Arts\", \"Engineering\", \"Science\", \" Arts\", \"Engineering\", \"Mathematics\", \"Science\", \"Arts\", \"Engineerings\", \"Science\", \"Mathematics\", \"Arts\", \"Engineerings\", \"Science\", \"Mathematics\", \"Arts\", \"Mathematiics\"]\n",
        "}\n",
        "\n",
        "# Création d'un DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Sauvegarde au format CSV\n",
        "df.to_csv(\"students.csv\", index=False)\n",
        "print(\"Fichier students.csv créé !\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgIp8berz8Gv"
      },
      "source": [
        "# Lecture du fichier CSV pour le traitement batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UE9J5CLgz-Rk",
        "outputId": "168d82ef-93ea-4eb5-9d2a-8d272a1427ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+---+------------+\n",
            "|    name|age|       major|\n",
            "+--------+---+------------+\n",
            "|   Alice| 22| Engineering|\n",
            "|     Bob| 20|     Science|\n",
            "| Charlie| 25| Mathematics|\n",
            "|   David| 19|        Arts|\n",
            "|     Eve| 24| Engineering|\n",
            "|   Frank| 23|     Science|\n",
            "|   Grace| 21|        Arts|\n",
            "|    Hank| 26| Engineering|\n",
            "|   Irene| 22| Mathematics|\n",
            "|    Jack| 20|     Science|\n",
            "|   Karen| 23|        Arts|\n",
            "|     Leo| 24|Engineerings|\n",
            "|     Mia| 21|     Science|\n",
            "|    Noah| 25| Mathematics|\n",
            "|     Tom| 22|        Arts|\n",
            "|   Lucie| 20|Engineerings|\n",
            "|   Clara| 23|     Science|\n",
            "|    Bill| 25| Mathematics|\n",
            "|    Carl| 18|        Arts|\n",
            "|Patricia| 20|Mathematiics|\n",
            "+--------+---+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Créer une session Spark\n",
        "spark = SparkSession.builder.appName(\"ETL_Pipeline\").getOrCreate()\n",
        "\n",
        "# Lire le fichier CSV\n",
        "df = spark.read.csv(\"students.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Afficher les données\n",
        "df.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDhQv2N50Fbe"
      },
      "source": [
        "# Applications des transformations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llR8qEVY0MGf"
      },
      "source": [
        "Convertissons les noms en majuscules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8WXPBXg0ITg",
        "outputId": "215372d7-5538-4d51-fdbb-b9ebea00e51f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+--------+---+------------+\n",
            "|    name|age|       major|\n",
            "+--------+---+------------+\n",
            "|   ALICE| 22| Engineering|\n",
            "|     BOB| 20|     Science|\n",
            "| CHARLIE| 25| Mathematics|\n",
            "|   DAVID| 19|        Arts|\n",
            "|     EVE| 24| Engineering|\n",
            "|   FRANK| 23|     Science|\n",
            "|   GRACE| 21|        Arts|\n",
            "|    HANK| 26| Engineering|\n",
            "|   IRENE| 22| Mathematics|\n",
            "|    JACK| 20|     Science|\n",
            "|   KAREN| 23|        Arts|\n",
            "|     LEO| 24|Engineerings|\n",
            "|     MIA| 21|     Science|\n",
            "|    NOAH| 25| Mathematics|\n",
            "|     TOM| 22|        Arts|\n",
            "|   LUCIE| 20|Engineerings|\n",
            "|   CLARA| 23|     Science|\n",
            "|    BILL| 25| Mathematics|\n",
            "|    CARL| 18|        Arts|\n",
            "|PATRICIA| 20|Mathematiics|\n",
            "+--------+---+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import upper\n",
        "\n",
        "df_transformed = df.withColumn(\"name\", upper(df[\"name\"]))\n",
        "df_transformed.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l-H5710x0Wou"
      },
      "source": [
        "Filtrons les étudiants de plus de 20 ans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cSI405MI0aJ3",
        "outputId": "9208688e-78d3-4e1e-e2e4-e9b0a2e648cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-------+---+------------+\n",
            "|   name|age|       major|\n",
            "+-------+---+------------+\n",
            "|  ALICE| 22| Engineering|\n",
            "|CHARLIE| 25| Mathematics|\n",
            "|    EVE| 24| Engineering|\n",
            "|  FRANK| 23|     Science|\n",
            "|  GRACE| 21|        Arts|\n",
            "|   HANK| 26| Engineering|\n",
            "|  IRENE| 22| Mathematics|\n",
            "|  KAREN| 23|        Arts|\n",
            "|    LEO| 24|Engineerings|\n",
            "|    MIA| 21|     Science|\n",
            "|   NOAH| 25| Mathematics|\n",
            "|    TOM| 22|        Arts|\n",
            "|  CLARA| 23|     Science|\n",
            "|   BILL| 25| Mathematics|\n",
            "+-------+---+------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_filtered = df_transformed.filter(df[\"age\"] > 20)\n",
        "df_filtered.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60Fb8vNH0g8F"
      },
      "source": [
        "Calculons la moyenne d'age"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmfLfXva0mUK",
        "outputId": "205baf5a-ef38-4d6d-cbdc-47fdefa8de26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+------------+------------------+\n",
            "|       major|          avg(age)|\n",
            "+------------+------------------+\n",
            "|     Science|22.333333333333332|\n",
            "| Engineering|              24.0|\n",
            "|        Arts|              21.0|\n",
            "|Engineerings|              24.0|\n",
            "|        Arts|              22.5|\n",
            "| Mathematics|             24.25|\n",
            "+------------+------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_filtered.groupBy(\"major\").avg(\"age\").show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sd_10ak71AkK"
      },
      "source": [
        "# **Simulation d'un flux en temps réel**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V13reQOzIGYg"
      },
      "outputs": [],
      "source": [
        "# prompt: mkdir /content/input_files\n",
        "# mv /content/disney_movies.csv /content/input_files/\n",
        "\n",
        "!mkdir /content/input_files\n",
        "!mv /content/disney_movies.csv /content/input_files/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5NCZjWZIOW7",
        "outputId": "3744f010-81ca-405e-df1a-f6be74d0fe96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- major: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# prompt: df_stream = spark.readStream.schema(schema).csv(\"/content/input_files/\", header=True)\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "# Define the schema for your CSV data\n",
        "schema = StructType([\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"age\", IntegerType(), True),\n",
        "    StructField(\"major\", StringType(), True)\n",
        "])\n",
        "\n",
        "# Create a SparkSession\n",
        "spark = SparkSession.builder.appName(\"StreamingCSV\").getOrCreate()\n",
        "\n",
        "# Read the stream\n",
        "df_stream = spark.readStream.schema(schema).csv(\"/content/input_files/\", header=True)\n",
        "\n",
        "# Print the schema\n",
        "df_stream.printSchema()\n",
        "\n",
        "# Start the query and print the results to the console\n",
        "query = df_stream.writeStream.outputMode(\"append\").format(\"console\").start()\n",
        "query.awaitTermination(10) # Wait for 10 seconds\n",
        "\n",
        "# Stop the query\n",
        "query.stop()\n",
        "\n",
        "# Stop the SparkSession\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        },
        "id": "RHx5TN1m1AOf",
        "outputId": "9f1ce87c-def4-4352-a835-5174d1f31323"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:root:KeyboardInterrupt while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 511, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-3637273fe73b>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# Attente de la fin du streaming\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/streaming/query.py\u001b[0m in \u001b[0;36mawaitTermination\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m                 \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    512\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m                 \u001b[0;31m# Happens when a the other end is dead. There might be an empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import upper\n",
        "\n",
        "# Créer une session Spark\n",
        "spark = SparkSession.builder.appName(\"StreamingExample\").getOrCreate()\n",
        "\n",
        "# Schéma (à ajuster selon votre CSV)\n",
        "schema = \"name STRING, age INT, grade STRING\"\n",
        "\n",
        "# Lire le flux depuis un répertoire\n",
        "df_stream = spark.readStream.schema(schema).csv(\"/content/input_files/\", header=True)\n",
        "\n",
        "# Transformation : convertir les noms en majuscules\n",
        "df_stream_transformed = df_stream.withColumn(\"name\", upper(df_stream[\"name\"]))\n",
        "\n",
        "# Écriture des résultats sur la console\n",
        "query = df_stream_transformed.writeStream.format(\"console\").outputMode(\"append\").start()\n",
        "\n",
        "# Attente de la fin du streaming\n",
        "query.awaitTermination()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_ZRMskk4qQcK"
      },
      "outputs": [],
      "source": [
        "!pip install kafka-python\n",
        "\n",
        "from kafka import KafkaProducer\n",
        "import json\n",
        "import time\n",
        "\n",
        "# Kafka configuration\n",
        "bootstrap_servers = 'kafka-494d166-paulallanmeyesika-3d16.h.aivencloud.com:16362'  # Replace with your Kafka broker address\n",
        "topic_name = 'examen'  # Replace with your desired topic name\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DS0m-d7UpqY"
      },
      "outputs": [],
      "source": [
        "# prompt: configuration du producer\n",
        "\n",
        "# Kafka configuration\n",
        "bootstrap_servers = 'kafka-494d166-paulallanmeyesika-3d16.h.aivencloud.com:16362'  # Replace with your Kafka broker address\n",
        "topic_name = 'examen'  # Replace with your desired topic name\n",
        "security_protocol = 'SSL'\n",
        "ssl_cafile = '/content/ca.pem' # Remplacez par le chemin vers votre fichier ca.pem\n",
        "ssl_certfile = '/content/service.cert' # Remplacez par le chemin vers votre fichier service.cert\n",
        "ssl_keyfile = '/content/service.key' # Remplacez par le chemin vers votre fichier service.key\n",
        "\n",
        "\n",
        "# Create a Kafka producer\n",
        "producer = KafkaProducer(\n",
        "    bootstrap_servers=bootstrap_servers,\n",
        "    security_protocol=security_protocol,\n",
        "    ssl_cafile=ssl_cafile,\n",
        "    ssl_certfile=ssl_certfile,\n",
        "    ssl_keyfile=ssl_keyfile,\n",
        "    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
        ")\n",
        "\n",
        "\n",
        "# Example usage (replace with your actual data)\n",
        "\n",
        "data = {\n",
        "    \"name\": [\"Alice\", \"Bob\", \"Charlie\", \"David\", \"Eve\", \"Frank\", \"Grace\", \"Hank\", \" Irene\", \"Jack\", \"Karen\", \"Leo\", \"Mia\", \"Noah\", \"Tom\", \"Lucie\", \"Clara\", \"Bill\", \"Carl\", \"Patricia\"],\n",
        "    \"age\": [22, 20, 25, 19, 24, 23, 21, 26, 22, 20, 23, 24, 21, 25, 22, 20, 23, 25, 18, 20],\n",
        "    \"major\": [\"Engineering\", \"Science\", \"Mathematics\", \"Arts\", \"Engineering\", \"Science\", \" Arts\", \"Engineering\", \"Mathematics\", \"Science\", \"Arts\", \"Engineerings\", \"Science\", \"Mathematics\", \"Arts\", \"Engineerings\", \"Science\", \"Mathematics\", \"Arts\", \"Mathematiics\"]\n",
        "}\n",
        "\n",
        "\n",
        "# Send the data to Kafka\n",
        "producer.send(topic_name, value=data)\n",
        "producer.flush() # Important : assurez-vous que les messages sont envoyés\n",
        "\n",
        "print(f\"Message sent to Kafka topic '{topic_name}'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejmWVS4sAMiv",
        "outputId": "48000477-bf6b-44c1-8454-08bdde911135"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Message sent to Kafka topic 'examen'\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import csv\n",
        "from kafka import KafkaProducer\n",
        "\n",
        "# Configuration Kafka\n",
        "bootstrap_servers = 'kafka-494d166-paulallanmeyesika-3d16.h.aivencloud.com:16362'  # Replace with your Kafka broker address\n",
        "topic_name = 'examen'  # Replace with your desired topic name\n",
        "security_protocol = 'SSL'\n",
        "ssl_cafile = '/content/ca.pem'  # Remplacez par le chemin vers votre fichier ca.pem\n",
        "ssl_certfile = '/content/service.cert'  # Remplacez par le chemin vers votre fichier service.cert\n",
        "ssl_keyfile = '/content/service.key'  # Remplacez par le chemin vers votre fichier service.key\n",
        "\n",
        "# Créer un Kafka producer\n",
        "producer = KafkaProducer(\n",
        "    bootstrap_servers=bootstrap_servers,\n",
        "    security_protocol=security_protocol,\n",
        "    ssl_cafile=ssl_cafile,\n",
        "    ssl_certfile=ssl_certfile,\n",
        "    ssl_keyfile=ssl_keyfile,\n",
        "    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
        ")\n",
        "\n",
        "# Chemin vers le fichier CSV\n",
        "csv_file_path = '/content/students.csv'  # Remplacez par le chemin réel de votre fichier CSV\n",
        "\n",
        "# Lire le fichier CSV et envoyer les données\n",
        "try:\n",
        "    with open(csv_file_path, 'r') as csvfile:\n",
        "        reader = csv.DictReader(csvfile)  # Utilisation de DictReader pour lire le CSV en tant que dictionnaire\n",
        "        for row in reader:\n",
        "            # Envoyer chaque ligne sous forme JSON à Kafka\n",
        "            producer.send(topic_name, value=row)\n",
        "            print(f\"Message sent: {row}\")\n",
        "\n",
        "    producer.flush()  # Assurez-vous que tous les messages sont envoyés\n",
        "    print(f\"All messages from '{csv_file_path}' have been sent to Kafka topic '{topic_name}'.\")\n",
        "except Exception as e:\n",
        "    print(f\"Message sent to Kafka topic '{topic_name}'\")\n",
        "finally:\n",
        "    producer.close()  # Toujours fermer le producteur\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tBK3Exat2RL"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-559Wz-t3Ev",
        "outputId": "a44c13c0-2bf3-4a50-de2c-cb56c4b528be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: kafka-python in /usr/local/lib/python3.10/dist-packages (2.0.2)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark kafka-python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbE4JEGCuGJ2"
      },
      "source": [
        "# Configuration Spark Streaming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWsYvzFXxarD"
      },
      "outputs": [],
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Kafka_Consumer\") \\\n",
        "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1\") \\\n",
        "    .getOrCreate()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPtdY_sFzo4z",
        "outputId": "d9de24f3-e67f-4e6d-d638-596be9398644"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "--2024-11-27 18:07:40--  https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.4.1/spark-sql-kafka-0-10_2.12-3.4.1.jar\n",
            "Resolving repo1.maven.org (repo1.maven.org)... 199.232.192.209, 199.232.196.209, 2a04:4e42:4c::209, ...\n",
            "Connecting to repo1.maven.org (repo1.maven.org)|199.232.192.209|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 427253 (417K) [application/java-archive]\n",
            "Saving to: ‘/jars/spark-sql-kafka-0-10_2.12-3.4.1.jar’\n",
            "\n",
            "spark-sql-kafka-0-1 100%[===================>] 417.24K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-11-27 18:07:40 (8.62 MB/s) - ‘/jars/spark-sql-kafka-0-10_2.12-3.4.1.jar’ saved [427253/427253]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark\n",
        "!wget https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.4.1/spark-sql-kafka-0-10_2.12-3.4.1.jar -P $SPARK_HOME/jars\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VX0Jk1qmHmwx",
        "outputId": "0caf0e86-1a1b-4680-af5f-81f55dd4932d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Une erreur est survenue : Failed to find data source: kafka. Please deploy the application as per the deployment section of Structured Streaming + Kafka Integration Guide.\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import upper\n",
        "\n",
        "# Initialiser une session Spark avec le connecteur Kafka\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Kafka_Consumer\") \\\n",
        "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Configuration du journal Spark (optionnel)\n",
        "spark.sparkContext.setLogLevel(\"DEBUG\")  # DEBUG ou INFO pour plus de détails\n",
        "\n",
        "try:\n",
        "    # Lire les données du topic Kafka en streaming\n",
        "    df_stream = spark \\\n",
        "        .readStream \\\n",
        "        .format(\"kafka\") \\\n",
        "        .option(\"kafka.bootstrap.servers\", \"kafka-494d166-paulallanmeyesika-3d16.h.aivencloud.com:16362\") \\\n",
        "        .option(\"subscribe\", \"examen\") \\\n",
        "        .load()\n",
        "\n",
        "    # Convertir les données Kafka en texte lisible\n",
        "    df_value = df_stream.selectExpr(\"CAST(value AS STRING) as value\")\n",
        "\n",
        "    # Appliquer des transformations (Exemple : majuscules pour les noms)\n",
        "    df_transformed = df_value.withColumn(\"value\", upper(df_value[\"value\"]))\n",
        "\n",
        "    # Afficher les données en temps réel dans la console\n",
        "    query = df_transformed.writeStream.format(\"console\").outputMode(\"append\").start()\n",
        "\n",
        "    # Attendre la fin du traitement\n",
        "    query.awaitTermination()\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Une erreur est survenue : {e}\")\n",
        "\n",
        "finally:\n",
        "    # Arrêter la session Spark proprement\n",
        "    spark.stop()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMLgzFA_J4yJ",
        "outputId": "9003d5c0-9675-45ac-f296-cabc5afeb6a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-rw-r--r-- 1 root root 1537 Nov 27 17:33 /content/ca.pem\n",
            "-rw-r--r-- 1 root root 1578 Nov 27 17:33 /content/service.cert\n",
            "-rw-r--r-- 1 root root 2484 Nov 27 17:33 /content/service.key\n"
          ]
        }
      ],
      "source": [
        "# Configuration Kafka (unchanged)\n",
        "bootstrap_servers = 'kafka-494d166-paulallanmeyesika-3d16.h.aivencloud.com:16362'\n",
        "topic_name = 'examen'\n",
        "security_protocol = 'SSL'\n",
        "ssl_cafile = '/content/ca.pem'\n",
        "ssl_certfile = '/content/service.cert'\n",
        "ssl_keyfile = '/content/service.key'\n",
        "\n",
        "# List Kafka files\n",
        "!ls -l /content/*.pem /content/*.cert /content/*.key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-9EMEkNKPj7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "zEPjL1Iq7ii3",
        "outputId": "2b14950e-858b-4327-fb8f-bd7165b6acb9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:kafka.coordinator.consumer:group_id is None: disabling auto-commit.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Received message: {'name': 'Alice', 'age': 30, 'major': 'Computer Science'}\n",
            "Received message: {'name': 'Alice', 'age': 22, 'major': ' Engineering'}\n",
            "Received message: {'name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve', 'Frank', 'Grace', 'Hank', ' Irene', 'Jack', 'Karen', 'Leo', 'Mia', 'Noah', 'Tom', 'Lucie', 'Clara', 'Bill', 'Carl', 'Patricia'], 'age': [22, 20, 25, 19, 24, 23, 21, 26, 22, 20, 23, 24, 21, 25, 22, 20, 23, 25, 18, 20], 'major': ['Engineering', 'Science', 'Mathematics', 'Arts', 'Engineering', 'Science', ' Arts', 'Engineering', 'Mathematics', 'Science', 'Arts', 'Engineerings', 'Science', 'Mathematics', 'Arts', 'Engineerings', 'Science', 'Mathematics', 'Arts', 'Mathematiics']}\n",
            "Received message: {'name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve', 'Frank', 'Grace', 'Hank', ' Irene', 'Jack', 'Karen', 'Leo', 'Mia', 'Noah', 'Tom', 'Lucie', 'Clara', 'Bill', 'Carl', 'Patricia'], 'age': [22, 20, 25, 19, 24, 23, 21, 26, 22, 20, 23, 24, 21, 25, 22, 20, 23, 25, 18, 20], 'major': ['Engineering', 'Science', 'Mathematics', 'Arts', 'Engineering', 'Science', ' Arts', 'Engineering', 'Mathematics', 'Science', 'Arts', 'Engineerings', 'Science', 'Mathematics', 'Arts', 'Engineerings', 'Science', 'Mathematics', 'Arts', 'Mathematiics']}\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-1627ddb3e881>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconsumer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Received message: {message.value}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/kafka/consumer/group.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1191\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_v1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnext_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/kafka/consumer/group.py\u001b[0m in \u001b[0;36mnext_v2\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1199\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_message_generator_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1201\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1202\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1203\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/kafka/consumer/group.py\u001b[0m in \u001b[0;36m_message_generator_v2\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_message_generator_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0mtimeout_ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consumer_timeout\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m         \u001b[0mrecord_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_offsets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1117\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecord_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m             \u001b[0;31m# Generators are stateful, and it is possible that the tp / records\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/kafka/consumer/group.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[1;32m    653\u001b[0m         \u001b[0mremaining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m             \u001b[0mrecords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_records\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_offsets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdate_offsets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/kafka/consumer/group.py\u001b[0m in \u001b[0;36m_poll_once\u001b[0;34m(self, timeout_ms, max_records, update_offsets)\u001b[0m\n\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m         \u001b[0mtimeout_ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_coordinator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_to_next_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m         \u001b[0;31m# after the long poll, we should check whether the group needs to rebalance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m         \u001b[0;31m# prior to returning data so that the group can stabilize faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/kafka/client_async.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout_ms, future)\u001b[0m\n\u001b[1;32m    600\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# avoid negative timeouts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0;31m# called without the lock to avoid deadlock potential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/kafka/client_async.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0mstart_select\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0mend_select\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 469\u001b[0;31m                 \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_ev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    470\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    471\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Code to create a Kafka consumer (replace existing incorrect code)\n",
        "from kafka import KafkaConsumer\n",
        "import json\n",
        "\n",
        "bootstrap_servers = 'kafka-494d166-paulallanmeyesika-3d16.h.aivencloud.com:16362'  # Replace with your Kafka broker address\n",
        "topic_name = 'examen'  # Replace with your desired topic name\n",
        "security_protocol = 'SSL'\n",
        "ssl_cafile = '/content/ca.pem' # Replace with the path to your ca.pem file\n",
        "ssl_certfile = '/content/service.cert' # Replace with the path to your service.cert file\n",
        "ssl_keyfile = '/content/service.key' # Replace with the path to your service.key file\n",
        "\n",
        "consumer = KafkaConsumer(\n",
        "    topic_name,\n",
        "    bootstrap_servers=bootstrap_servers,\n",
        "    security_protocol=security_protocol,\n",
        "    ssl_cafile=ssl_cafile,\n",
        "    ssl_certfile=ssl_certfile,\n",
        "    ssl_keyfile=ssl_keyfile,\n",
        "    value_deserializer=lambda x: json.loads(x.decode('utf-8')),\n",
        "    auto_offset_reset='earliest', # Start reading from the beginning of the topic\n",
        "    enable_auto_commit=True, # Enable auto commit of offsets\n",
        ")\n",
        "\n",
        "for message in consumer:\n",
        "    print(f\"Received message: {message.value}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}