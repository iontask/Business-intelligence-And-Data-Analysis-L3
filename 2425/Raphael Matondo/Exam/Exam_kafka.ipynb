{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, upper, avg, round\n",
        "spark = SparkSession.builder.appName(\"MyApp\").getOrCreate()\n",
        "print(\"Connexion réussie avec Pyspark !\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLkDZEDyARph",
        "outputId": "1f200237-3033-4248-e1ea-b6cc7d03a236"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Connexion réussie avec Pyspark !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lire un fichier CSV\n",
        "df = spark.read.csv(\"/content/drive/MyDrive/students.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Afficher le schéma et les premières lignes\n",
        "df.printSchema()\n",
        "df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zC2DAQH9Uv7q",
        "outputId": "c59209f9-ab47-424c-811a-dba1e88e031c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- nom: string (nullable = true)\n",
            " |-- prenom: string (nullable = true)\n",
            " |-- age: integer (nullable = true)\n",
            " |-- filiere: string (nullable = true)\n",
            "\n",
            "+--------+------+---+--------+\n",
            "|     nom|prenom|age| filiere|\n",
            "+--------+------+---+--------+\n",
            "|Williams|  Paul| 25|Biologie|\n",
            "|   Jones| Marie| 19|Biologie|\n",
            "|   Brown| Marie| 18|Biologie|\n",
            "|  Garcia| Emily| 27|Biologie|\n",
            "|   Jones|  John| 29|Histoire|\n",
            "+--------+------+---+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformation 1 : Convertir les noms des étudiants en majuscules\n",
        "df_transformed = df.withColumn(\"nom\", upper(col(\"nom\")))\n",
        "print(\"Après conversion des noms en majuscules :\")\n",
        "df_transformed.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnQYKAXWUvd9",
        "outputId": "ad579a8d-90b8-4279-86c8-610de79ff330"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Après conversion des noms en majuscules :\n",
            "+--------+------+---+--------+\n",
            "|     nom|prenom|age| filiere|\n",
            "+--------+------+---+--------+\n",
            "|WILLIAMS|  Paul| 25|Biologie|\n",
            "|   JONES| Marie| 19|Biologie|\n",
            "|   BROWN| Marie| 18|Biologie|\n",
            "|  GARCIA| Emily| 27|Biologie|\n",
            "|   JONES|  John| 29|Histoire|\n",
            "+--------+------+---+--------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformation 2 : Filtrer les étudiants ayant plus de 20 ans\n",
        "df_filtered = df_transformed.filter(col(\"age\") > 20)\n",
        "print(\"Après filtrage des étudiants ayant plus de 20 ans :\")\n",
        "df_filtered.show(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfExi5vPYxdG",
        "outputId": "e432cde2-9e27-4b9e-aa9d-6a58c608c5fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Après filtrage des étudiants ayant plus de 20 ans :\n",
            "+--------+-------+---+------------+\n",
            "|     nom| prenom|age|     filiere|\n",
            "+--------+-------+---+------------+\n",
            "|WILLIAMS|   Paul| 25|    Biologie|\n",
            "|  GARCIA|  Emily| 27|    Biologie|\n",
            "|   JONES|   John| 29|    Histoire|\n",
            "|   SMITH| Sophia| 23|    Biologie|\n",
            "|  GARCIA|Michael| 30|    Économie|\n",
            "|WILLIAMS|  Marie| 23|      Chimie|\n",
            "|   BROWN| Sophia| 26|Informatique|\n",
            "| JOHNSON|  David| 27|    Physique|\n",
            "|  MILLER| Sophia| 28|    Histoire|\n",
            "|   JONES|   Anna| 21| Littérature|\n",
            "+--------+-------+---+------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformation 3 : Calculer la moyenne d'âge par filière et l'arrondir\n",
        "df_stats = df_filtered.groupBy(\"filiere\").agg(round(avg(\"age\"), 1).alias(\"moyenne_age\"))\n",
        "print(\"Moyenne d'âge par filière (arrondie) :\")\n",
        "df_stats.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GocBJOdZJE7",
        "outputId": "c71464a7-0fd5-4cfe-b01b-fb82e64baf1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moyenne d'âge par filière (arrondie) :\n",
            "+-------------+-----------+\n",
            "|      filiere|moyenne_age|\n",
            "+-------------+-----------+\n",
            "| Informatique|       25.9|\n",
            "|       Chimie|       25.5|\n",
            "|Mathématiques|       25.9|\n",
            "|     Économie|       25.5|\n",
            "|     Physique|       25.9|\n",
            "|     Histoire|       25.6|\n",
            "|  Philosophie|       26.1|\n",
            "|  Littérature|       25.7|\n",
            "|     Biologie|       26.0|\n",
            "+-------------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_transformed.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K37bB4Fle6S1",
        "outputId": "11d7a9bd-599d-44a4-a9cc-d9e6a5773305"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+-------+---+-------------+\n",
            "|     nom| prenom|age|      filiere|\n",
            "+--------+-------+---+-------------+\n",
            "|WILLIAMS|   Paul| 25|     Biologie|\n",
            "|   JONES|  Marie| 19|     Biologie|\n",
            "|   BROWN|  Marie| 18|     Biologie|\n",
            "|  GARCIA|  Emily| 27|     Biologie|\n",
            "|   JONES|   John| 29|     Histoire|\n",
            "| JOHNSON|   Paul| 18| Informatique|\n",
            "|   SMITH| Sophia| 18| Informatique|\n",
            "|     DOE|Michael| 18|  Philosophie|\n",
            "|   SMITH| Sophia| 23|     Biologie|\n",
            "|  GARCIA|Michael| 30|     Économie|\n",
            "|WILLIAMS|  Marie| 23|       Chimie|\n",
            "|   BROWN| Sophia| 26| Informatique|\n",
            "| JOHNSON|  David| 27|     Physique|\n",
            "|  MILLER| Sophia| 28|     Histoire|\n",
            "|   JONES|   Anna| 21|  Littérature|\n",
            "|   SMITH|   John| 24|     Biologie|\n",
            "|  MILLER|   Jane| 24|     Biologie|\n",
            "|   SMITH|   Jane| 19|  Littérature|\n",
            "|   NGOMA|  David| 26|     Biologie|\n",
            "|   BROWN|Michael| 27|Mathématiques|\n",
            "+--------+-------+---+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sauvegarder les données transformées dans un fichier CSV localement dans Colab\n",
        "output_path_local = \"/content/students_transformed.csv\"  # Emplacement temporaire dans l'environnement Colab\n",
        "\n",
        "# Sauvegarder le DataFrame transformé en CSV\n",
        "df_transformed.coalesce(1).write.csv(output_path_local, header=True, mode=\"overwrite\")\n",
        "\n",
        "print(f\"Données transformées sauvegardées avec succès\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GwjdLuUTbr_d",
        "outputId": "c7f35185-680d-45d2-a6cf-95a6398bc21c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Données transformées sauvegardées avec succès\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Installation des dépendances nécessaires\n",
        "!pip install pyspark confluent_kafka\n",
        "\n",
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, upper\n",
        "from confluent_kafka import Producer, Consumer, KafkaError\n",
        "\n",
        "# Définir les chemins des certificats SSL\n",
        "ssl_cafile = \"/content/drive/MyDrive/ca.pem\"\n",
        "ssl_certfile = \"/content/drive/MyDrive/service.cert\"\n",
        "ssl_keyfile = \"/content/drive/MyDrive/service.key\"\n",
        "\n",
        "# Vérification des fichiers de certificat\n",
        "print(\"CA File exists:\", os.path.exists(ssl_cafile))\n",
        "print(\"Cert File exists:\", os.path.exists(ssl_certfile))\n",
        "print(\"Key File exists:\", os.path.exists(ssl_keyfile))\n",
        "\n",
        "# Étape 1 : Lecture et transformation des données avec PySpark\n",
        "spark = SparkSession.builder.appName(\"KafkaSparkPipeline\").getOrCreate()\n",
        "print(\"Connexion réussie avec Aiven kafka !\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7o87uAqGsm0",
        "outputId": "40e2785a-bf5c-437f-fed8-4fac8a13bde4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Collecting confluent_kafka\n",
            "  Downloading confluent_kafka-2.6.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Downloading confluent_kafka-2.6.1-cp310-cp310-manylinux_2_28_x86_64.whl (3.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: confluent_kafka\n",
            "Successfully installed confluent_kafka-2.6.1\n",
            "CA File exists: True\n",
            "Cert File exists: True\n",
            "Key File exists: True\n",
            "Connexion réussie avec Aiven kafka !\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from confluent_kafka import Consumer, KafkaError\n",
        "\n",
        "# Initialisation de Spark\n",
        "spark = SparkSession.builder.master(\"local[*]\").appName(\"KafkaPipeline\").getOrCreate()\n",
        "\n",
        "# Charger les données CSV\n",
        "input_csv_path = \"/content/drive/MyDrive/students.csv\"\n",
        "df = spark.read.csv(input_csv_path, header=True, inferSchema=True)\n",
        "\n",
        "# Prendre les 10 premières lignes\n",
        "df = df.limit(10)\n",
        "\n",
        "# Afficher les 10 premiers étudiants sélectionnés\n",
        "print(\"10 premiers étudiants sélectionnés :\")\n",
        "df.show()\n",
        "\n",
        "# Configuration du consommateur Kafka\n",
        "consumer_conf = {\n",
        "    'bootstrap.servers': 'kafka-1a2fc4df-mvuezoloraphael0-6122.g.aivencloud.com:11343',\n",
        "    'security.protocol': 'SSL',\n",
        "    'ssl.ca.location': ssl_cafile,\n",
        "    'ssl.certificate.location': ssl_certfile,\n",
        "    'ssl.key.location': ssl_keyfile,\n",
        "    'group.id': 'student-consumer',\n",
        "    'auto.offset.reset': 'earliest'\n",
        "}\n",
        "\n",
        "# Créer le consommateur Kafka\n",
        "consumer = Consumer(consumer_conf)\n",
        "consumer.subscribe(['Goro'])\n",
        "\n",
        "# Liste pour stocker les messages consommés\n",
        "consumed_data = []\n",
        "message_count = 0\n",
        "\n",
        "# Afficher les messages envoyés à Goro dans le format voulu\n",
        "print(\"\\nMessages envoyés à Goro sont :\")\n",
        "try:\n",
        "    while message_count < 5:  # Consommer exactement 5 messages\n",
        "        msg = consumer.poll(1.0)  # Attente d'une seconde\n",
        "        if msg is None:\n",
        "            continue\n",
        "        if msg.error():\n",
        "            if msg.error().code() == KafkaError._PARTITION_EOF:\n",
        "                print('Fin des messages\\n')\n",
        "                break\n",
        "            else:\n",
        "                print(f'Erreur: {msg.error()}')\n",
        "                break\n",
        "\n",
        "        # Ajouter le message consommé à la liste\n",
        "        message = msg.value().decode('utf-8')\n",
        "        print(f\"Message brut reçu envoyé à Goro : {message}\")\n",
        "\n",
        "        # Décomposer le message reçu\n",
        "        message_data = message.strip(\"[]\").split(\",\")  # Supprimer les crochets et séparer par les virgules\n",
        "\n",
        "        if len(message_data) != 4:\n",
        "            print(f\"Format inattendu pour le message : {message_data}. Ignoré.\")\n",
        "            continue  # Ignorer les messages mal formés\n",
        "\n",
        "        nom, prenom, age, filiere = message_data\n",
        "\n",
        "        try:\n",
        "            # Convertir l'âge en entier\n",
        "            age = int(age.strip())  # Nettoyer les espaces et convertir en entier\n",
        "        except ValueError:\n",
        "            print(f\"Erreur de conversion de l'âge pour {nom} {prenom}. Ignoré.\")\n",
        "            continue  # Passer au message suivant si l'âge n'est pas un entier\n",
        "\n",
        "        # Appliquer le filtrage (âge > 20) et la transformation\n",
        "        if age > 20:\n",
        "            transformed_message = [nom.upper(), prenom, age, filiere]\n",
        "            consumed_data.append(transformed_message)\n",
        "            message_count += 1\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Erreur pendant la consommation : {e}\")\n",
        "finally:\n",
        "    consumer.close()\n",
        "\n",
        "# Afficher les messages consommés et transformés\n",
        "print(\"\\nMessages consommés et transformés sont :\")\n",
        "for transformed_message in consumed_data:\n",
        "    print(f\"Message consommé et transformé : {transformed_message}\")\n",
        "\n",
        "# Espacer les sections\n",
        "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
        "\n",
        "# Afficher le nombre de messages transformés\n",
        "print(f\"{message_count} messages consommés et transformés.\\n\")\n",
        "\n",
        "# Étape 4 : Sauvegarder les données consommées dans un fichier CSV\n",
        "if message_count > 0:\n",
        "    output_csv_path = \"/content/students_transformed_by_kafka.csv\"\n",
        "    columns = [\"nom\", \"prenom\", \"age\", \"filiere\"]  # Correspond à vos colonnes réelles\n",
        "\n",
        "    # Fusionner toutes les partitions en une seule et écrire un fichier unique\n",
        "    consumed_df = spark.createDataFrame(consumed_data, columns)\n",
        "    consumed_df.coalesce(1).write.csv(output_csv_path, header=True, mode=\"overwrite\")\n",
        "\n",
        "    print(f\"Données consommées sauvegardées dans : {output_csv_path}\")\n",
        "else:\n",
        "    print(\"Aucun message ne correspond aux critères (âge > 20). Aucun fichier sauvegardé.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_2VBpu1HGDr",
        "outputId": "8b0445d7-4874-4556-9d10-e130329f944d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 premiers étudiants sélectionnés :\n",
            "+--------+-------+---+------------+\n",
            "|     nom| prenom|age|     filiere|\n",
            "+--------+-------+---+------------+\n",
            "|Williams|   Paul| 25|    Biologie|\n",
            "|   Jones|  Marie| 19|    Biologie|\n",
            "|   Brown|  Marie| 18|    Biologie|\n",
            "|  Garcia|  Emily| 27|    Biologie|\n",
            "|   Jones|   John| 29|    Histoire|\n",
            "| Johnson|   Paul| 18|Informatique|\n",
            "|   Smith| Sophia| 18|Informatique|\n",
            "|     Doe|Michael| 18| Philosophie|\n",
            "|   Smith| Sophia| 23|    Biologie|\n",
            "|  Garcia|Michael| 30|    Économie|\n",
            "+--------+-------+---+------------+\n",
            "\n",
            "\n",
            "Messages envoyés à Goro sont :\n",
            "Message brut reçu envoyé à Goro : [Williams, Paul, 25, Biologie]\n",
            "Message brut reçu envoyé à Goro : [Jones, Marie, 19, Biologie]\n",
            "Message brut reçu envoyé à Goro : [Brown, Marie, 18, Biologie]\n",
            "Message brut reçu envoyé à Goro : [Garcia, Emily, 27, Biologie]\n",
            "Message brut reçu envoyé à Goro : [Jones, John, 29, Histoire]\n",
            "Message brut reçu envoyé à Goro : [Johnson, Paul, 18, Informatique]\n",
            "Message brut reçu envoyé à Goro : [Smith, Sophia, 18, Informatique]\n",
            "Message brut reçu envoyé à Goro : [Doe, Michael, 18, Philosophie]\n",
            "Message brut reçu envoyé à Goro : [Smith, Sophia, 23, Biologie]\n",
            "Message brut reçu envoyé à Goro : [Garcia, Michael, 30, Économie]\n",
            "\n",
            "Messages consommés et transformés sont :\n",
            "Message consommé et transformé : ['WILLIAMS', ' Paul', 25, ' Biologie']\n",
            "Message consommé et transformé : ['GARCIA', ' Emily', 27, ' Biologie']\n",
            "Message consommé et transformé : ['JONES', ' John', 29, ' Histoire']\n",
            "Message consommé et transformé : ['SMITH', ' Sophia', 23, ' Biologie']\n",
            "Message consommé et transformé : ['GARCIA', ' Michael', 30, ' Économie']\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "5 messages consommés et transformés.\n",
            "\n",
            "Données consommées sauvegardées dans : /content/students_transformed_by_kafka.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Merci pour votre attention Mr #"
      ],
      "metadata": {
        "id": "L1zjSzU_8bt5"
      }
    }
  ]
}