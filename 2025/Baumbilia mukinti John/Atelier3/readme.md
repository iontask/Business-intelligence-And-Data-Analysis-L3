```markdown
# Pipeline ETL et Streaming avec PySpark & Kafka

## üìã Table des Mati√®res
- [Installation](#installation)
- [Configuration](#configuration)
- [Pipeline ETL](#pipeline-etl)
- [Streaming](#streaming)
- [Am√©liorations](#am√©liorations)

## üõ† Installation
```bash
# Spark & Hadoop
wget https://dlcdn.apache.org/spark/spark-3.5.3/spark-3.5.3-bin-hadoop3.tgz
tar xf spark-3.4.1-bin-hadoop3.tgz

# D√©pendances Python
pip install findspark
```

## ‚öôÔ∏è Configuration

### Variables d'Environnement
```python
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.4.1-bin-hadoop3"
```

### Initialisation SparkSession
```python
import findspark
findspark.init()

spark = SparkSession.builder \
    .master("local[*]") \
    .appName("ETL_Pipeline") \
    .getOrCreate()
```

## üìä Pipeline ETL

### 1. Extraction
```python
# Chargement donn√©es
df = spark.read.csv(file_path, 
    header=True, 
    inferSchema=True)
```

### 2. Transformation
```python
# Noms en majuscules
df_transformed = df.withColumn("Nom", upper(col("Nom")))

# Filtrage √¢ge
df_filtered = df_transformed.filter(col("Age") > 20)

# Statistiques par fili√®re
df_statistics = df_filtered.groupBy("Fili√®re") \
    .agg(avg("Age").alias("Moyenne_Age"))
```

### 3. Chargement
```python
# Export donn√©es
df_filtered.write.csv("/content/etudiants_transformes.csv", 
    header=True, 
    mode="overwrite")

# Export statistiques
df_statistics.write.csv("/content/statistiques_filiere.csv", 
    header=True, 
    mode="overwrite")
```

## üîÑ Streaming

### Simulation Kafka Producer
```python
def simulate_kafka_producer(df_csv, output_path):
    with open(output_path, "w") as f:
        for row in df_csv.collect():
            record = {
                "Nom": row["Nom"],
                "Pr√©nom": row["Pr√©nom"],
                "Age": row["Age"],
                "Fili√®re": row["Fili√®re"]
            }
            json.dump(record, f)
            f.write("\n")
            time.sleep(2)
```

### Structured Streaming
```python
# Lecture stream
df = spark.readStream.text(output_path)

# Transformations
df_transformed = df.withColumn("Nom", upper(col("value")))

# Output
query = df_transformed.writeStream \
    .outputMode("append") \
    .format("console") \
    .start()
```

## üöÄ Am√©liorations Possibles

### Performance
- Optimisation m√©moire Spark
- Partitionnement donn√©es
- Configuration executors

### Robustesse
- Gestion erreurs
- Validation donn√©es
- Tests unitaires

### Monitoring
- M√©triques performance
- Logging d√©taill√©
- Alertes temps r√©el

## üìù Notes Techniques

### Pr√©requis
- Java 8+
- Python 3.7+
- Spark 3.4.1
- Hadoop 3

### Compatibilit√©
- Test√© sur Google Colab
- Compatible Linux/MacOS
- Support Windows limit√©


## üìÑ License
Open

```

Ce README fournit une documentation compl√®te et structur√©e du projet, adapt√©e pour GitHub. Les sections couvrent l'installation, la configuration, le pipeline ETL, et le streaming, avec des exemples de code et des recommandations d'am√©lioration.
